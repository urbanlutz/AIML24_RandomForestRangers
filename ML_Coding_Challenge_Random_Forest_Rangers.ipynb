{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ML Coding Challenge\n",
    "## Random Forest Rangers\n",
    "## Urban, Atilla, Jano"
   ],
   "metadata": {
    "id": "H24hLkOfNP8Z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "id": "yZtLM40ANoYV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "id": "uFUdT5EvNjh5",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:47:07.487443Z",
     "start_time": "2024-03-26T13:47:04.313237Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Training & Test Data"
   ],
   "metadata": {
    "id": "8PSMPxqggzmV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# mount g-drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# file locations\n",
    "DATA_PATH = \"drive/MyDrive/AIML24\"\n",
    "TRAINING_PATH = DATA_PATH + \"/remote_sensing/otherDatasets/sentinel_2/tif\"\n",
    "TEST_PATH = DATA_PATH + \"/remote_sensing/otherDatasets/sentinel_2/testset\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kE3L__QDgYvS",
    "outputId": "0972c115-aeec-409f-d273-271d4310aa7d",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:47:07.505475Z",
     "start_time": "2024-03-26T13:47:07.491462Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Download Training Data\n",
    "# !wget https://madm.dfki.de/files/sentinel/EuroSATallBands.zip --no-check-certificate\n",
    "# !unzip EuroSATallBands.zip\n",
    "# !mv ds/* \"drive/MyDrive/AIML24\"\n",
    "\n",
    "# ☝️Test data must be downloaded manually: put into AIML24/remote_sensing/otherDatasets/sentinel_2/testdata dir"
   ],
   "metadata": {
    "id": "0SYpaK22lHsC",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:47:07.519630Z",
     "start_time": "2024-03-26T13:47:07.509853Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "mj--r_py8lxr",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:47:07.530864Z",
     "start_time": "2024-03-26T13:47:07.524822Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "epyEbn8V8wFT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "M3fytIq68wau"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !unzip \"drive/MyDrive/AIML24/remote_sensing/otherDatasets/sentinel_2/testset.zip\"\n",
    "# !mv testset/testset/* \"drive/MyDrive/AIML24/remote_sensing/otherDatasets/sentinel_2/testset\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKZuqiBL7c3k",
    "outputId": "ae678dba-6edc-4c1c-9b78-e4990cda2484",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:47:07.546862Z",
     "start_time": "2024-03-26T13:47:07.538399Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define PyTorch Dataset"
   ],
   "metadata": {
    "id": "dq3pTziUr1MK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "len(glob.glob(os.path.join(TEST_PATH,  f\"*.npy\")))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "un57p0ZwhlaK",
    "outputId": "d19036bb-eeb1-49da-8749-11623231f302",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:13.256735Z",
     "start_time": "2024-03-26T13:48:13.247073Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install rasterio"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAZ7aZePhbJo",
    "outputId": "0e5065b6-e6e0-425b-984b-3fbb829875d8",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:32.891115Z",
     "start_time": "2024-03-26T13:48:13.419490Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rasterio\n",
      "  Downloading rasterio-1.3.9-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting affine (from rasterio)\n",
      "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs in c:\\users\\urban\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rasterio) (23.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\urban\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rasterio) (2024.2.2)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\urban\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rasterio) (8.1.7)\n",
      "Collecting cligj>=0.5 (from rasterio)\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\urban\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rasterio) (1.26.4)\n",
      "Collecting snuggs>=1.4.1 (from rasterio)\n",
      "  Downloading snuggs-1.4.7-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting click-plugins (from rasterio)\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\urban\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rasterio) (69.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\urban\\appdata\\roaming\\python\\python312\\site-packages (from click>=4.0->rasterio) (0.4.6)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in c:\\users\\urban\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from snuggs>=1.4.1->rasterio) (3.1.1)\n",
      "Downloading rasterio-1.3.9-cp312-cp312-win_amd64.whl (23.4 MB)\n",
      "   ---------------------------------------- 0.0/23.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/23.4 MB 1.7 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.6/23.4 MB 6.8 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.0/23.4 MB 9.4 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.0/23.4 MB 9.4 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.0/23.4 MB 9.4 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.0/23.4 MB 9.4 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.0/23.4 MB 9.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.3/23.4 MB 3.5 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.1/23.4 MB 5.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.1/23.4 MB 5.3 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.1/23.4 MB 6.3 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.1/23.4 MB 6.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.6/23.4 MB 5.8 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.2/23.4 MB 6.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.2/23.4 MB 6.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.2/23.4 MB 7.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.2/23.4 MB 7.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.2/23.4 MB 7.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.3/23.4 MB 7.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.3/23.4 MB 7.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 6.9/23.4 MB 7.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.3/23.4 MB 7.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 7.8/23.4 MB 7.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.4/23.4 MB 7.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.4/23.4 MB 7.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 9.4/23.4 MB 7.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 9.4/23.4 MB 7.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.1/23.4 MB 7.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.5/23.4 MB 8.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.5/23.4 MB 8.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.5/23.4 MB 8.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.5/23.4 MB 8.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 11.5/23.4 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 11.5/23.4 MB 8.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 11.7/23.4 MB 8.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 12.6/23.4 MB 8.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 12.6/23.4 MB 8.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 12.7/23.4 MB 8.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 12.7/23.4 MB 8.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 12.7/23.4 MB 8.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 12.7/23.4 MB 7.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 12.8/23.4 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.1/23.4 MB 6.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 13.7/23.4 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 14.4/23.4 MB 7.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 15.7/23.4 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 16.8/23.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 16.8/23.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 16.8/23.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 16.8/23.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 16.8/23.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 17.8/23.4 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 17.8/23.4 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 18.4/23.4 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 18.9/23.4 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.2/23.4 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 19.9/23.4 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 19.9/23.4 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.0/23.4 MB 8.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.0/23.4 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 21.2/23.4 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.0/23.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.0/23.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.3/23.4 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.3/23.4 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 23.4/23.4 MB 9.9 MB/s eta 0:00:00\n",
      "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Installing collected packages: snuggs, affine, cligj, click-plugins, rasterio\n",
      "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.3.9 snuggs-1.4.7\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import rasterio as rio\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "from collections.abc import Callable\n",
    "\n",
    "\n",
    "def load_img(img_path:str) -> np.ndarray:\n",
    "  if img_path.split('.')[-1] == \"tif\":\n",
    "    with rio.open(img_path, \"r\") as d:\n",
    "      img = d.read([1,2,3,4,5,6,7,8,9,10,11,12,13])\n",
    "      # Assuming bands 2, 3, 4 are the RGB channels (1-based indexing in rasterio)\n",
    "      # Adjust the indices as necessary based on your data\n",
    "      # img = d.read([2, 3, 4])\n",
    "      img = reshape_as_image(img)\n",
    "  else:\n",
    "    img = np.load(img_path)\n",
    "  return img\n",
    "\n",
    "class SentinelTrain(Dataset):\n",
    "    def __init__(self, transformations=None):\n",
    "        self.img_paths = glob.glob(os.path.join(TRAINING_PATH, \"*\", f\"*.tif\")) # TODO: remove data limit\n",
    "        # self.img_paths = glob.glob(os.path.join(TRAINING_PATH, \"*\", f\"*.tif\"))\n",
    "        folder_names = [f.split(\"/\")[-1] for f in glob.glob(TRAINING_PATH + \"/*\")]\n",
    "        self.label2ids = {name: id for (id, name) in enumerate(sorted(folder_names))}\n",
    "        self.ids2label = {v:k for k, v in self.label2ids.items()}\n",
    "        self.transformations = transformations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = load_img(img_path)\n",
    "        \n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "        \n",
    "        label = img_path.split(\"/\")[-1].split(\"_\")[0]\n",
    "        one_hot = self.text2oh(label)\n",
    "        return image, one_hot\n",
    "    \n",
    "    def text2oh(self, label):\n",
    "        one_hot = np.zeros(10)\n",
    "        one_hot.put(self.label2ids[label], 1)\n",
    "        return one_hot\n",
    "    \n",
    "    def oh2text(self, one_hot):\n",
    "        idx = np.argmax(one_hot)\n",
    "        return self.ids2label[idx]\n",
    "    \n",
    "def get_id(img_path):\n",
    "    return img_path.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "class SentinelTest(Dataset):\n",
    "    def __init__(self, transformations=None):\n",
    "        self.img_paths = glob.glob(os.path.join(TEST_PATH,  f\"*.npy\"))\n",
    "        self.transformations = transformations\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = load_img(img_path)\n",
    "        image_id = get_id(img_path)\n",
    "        \n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "        return image, image_id\n",
    "    "
   ],
   "metadata": {
    "id": "U6j3IRM6gghb",
    "ExecuteTime": {
     "end_time": "2024-03-26T15:57:18.582403Z",
     "start_time": "2024-03-26T15:57:18.566479Z"
    }
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create instances of the Dataset Class for both train & test\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from rasterio.plot import reshape_as_image\n",
    "\n",
    "\n",
    "def bandselect(img):\n",
    "    return img[:, :, [1,2,3]]\n",
    "\n",
    "def convert_to_float(img):\n",
    "    return img.astype(np.float32) / 10000.0\n",
    "\n",
    "\n",
    "# TODO: improvement -> find global max / min\n",
    "def l2a_approx(img):\n",
    "    l2a_bands = img[:,:,[0,1,2,3,4,5,6,7,8,9,11,12]]\n",
    "    band_min = np.min(l2a_bands, (0,1)) # minimal value per band\n",
    "    return l2a_bands - band_min # dark object subtraction algo approximation\n",
    "\n",
    "# Define the transformation functions as lambdas or wrap in Lambda transform\n",
    "train_transforms = transforms.Compose([\n",
    "    l2a_approx,\n",
    "    bandselect,\n",
    "    convert_to_float\n",
    "    #   if transform.ToTensor() is active, the error with the type appears\n",
    "    #   transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = SentinelTrain(train_transforms)\n",
    "\n",
    "\n",
    "test_transforms  = transforms.Compose([\n",
    "    bandselect,\n",
    "    convert_to_float\n",
    "\n",
    "#   if transform.ToTensor() is active, the error with the type appears\n",
    "#   transforms.ToTensor(),\n",
    "\n",
    "])\n",
    "test_dataset = SentinelTest(test_transforms)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgWl6XyfnOrH",
    "outputId": "f49a2fd0-f42e-4de0-f42d-499cc5cec180",
    "ExecuteTime": {
     "end_time": "2024-03-26T15:57:21.365738Z",
     "start_time": "2024-03-26T15:57:21.184984Z"
    }
   },
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27000\n",
      "4232\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataset.__getitem__(2450)[1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "s54yiq2zlAXy",
    "outputId": "f3690ba6-ef56-4994-c57d-be8e6c01cd9d",
    "ExecuteTime": {
     "end_time": "2024-03-26T15:57:23.530996Z",
     "start_time": "2024-03-26T15:57:23.515796Z"
    }
   },
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "'3202'"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n"
   ],
   "metadata": {
    "id": "uJEWU_uJhf3E",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:46.034706Z",
     "start_time": "2024-03-26T13:48:46.033705Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Utility function to display image from dataset\n",
    "from rasterio.plot import reshape_as_image\n",
    "\n",
    "def normalize_for_display(band_data):\n",
    "    \"\"\"Normalize multi-spectral imagery across bands.\n",
    "    The input is expected to be in HxWxC format, e.g. 64x64x13.\n",
    "    To account for outliers (e.g. extremly high values due to\n",
    "    reflective surfaces), we normalize with the 2- and 98-percentiles\n",
    "    instead of minimum and maximum of each band.\n",
    "    \"\"\"\n",
    "    band_data = np.array(band_data)\n",
    "    lower_perc = np.percentile(band_data, 2, axis=(0,1))\n",
    "    upper_perc = np.percentile(band_data, 98, axis=(0,1))\n",
    "    print(lower_perc)\n",
    "    print(upper_perc)\n",
    "    return (band_data - lower_perc) / (upper_perc - lower_perc)\n",
    "\n",
    "\n",
    "def print_image(img, label, rgb_bands=[3,2,1])-> None:\n",
    "  \"\"\"Displays an image. Indices of bands given by \"rgb_bands\" will be displayed as RGB in the print\n",
    "  \"\"\"\n",
    "  normalized_img = normalize_for_display(img)\n",
    "  rgb_img = normalized_img[:, :, rgb_bands]\n",
    "#   rgb_img = img[:, :, rgb_bands]\n",
    "  fig, ax = plt.subplots(1, figsize=(5,5))\n",
    "  ax.imshow((rgb_img * 255).astype(np.uint8), vmin=0, vmax=255)\n",
    "#   ax.imshow(rgb_img)\n",
    "  try:\n",
    "    ax.set_title(f\"{train_dataset.ids2label[label]}\")\n",
    "  except:\n",
    "    ax.set_title(f\"unknown\")\n",
    "  ax.axis(False)\n",
    "  plt.tight_layout()\n",
    "\n",
    "  plt.show()"
   ],
   "metadata": {
    "id": "GJYIs5jInbbx",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:46.039237Z",
     "start_time": "2024-03-26T13:48:46.038036Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Example image from train\n",
    "img, label = train_dataset.__getitem__(3001)\n",
    "print_image(img, label, [0,1,2])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "zmFUg37hoQdf",
    "outputId": "6d98d42d-dd25-4c07-f18a-27cafcc41eca",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:46.043862Z",
     "start_time": "2024-03-26T13:48:46.042405Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# example image from test\n",
    "img, label = test_dataset.__getitem__(800)\n",
    "print_image(img, label, [0,1,2])\n"
   ],
   "metadata": {
    "id": "rjQNNMvJrBsA",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "outputId": "a5ddd38c-ce39-4bbd-b972-9cd6a5b48eb2",
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:46.048694Z",
     "start_time": "2024-03-26T13:48:46.047399Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "id": "uDpb4f0SNs6x"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "img.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ooY1wBiCWBN",
    "outputId": "c4766e1c-c276-4a24-a043-ac37baa67a8f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "train model"
   ],
   "metadata": {
    "id": "zFnJNk9nTCnk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchsummary import torchsummary\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        # Load a pre-trained ResNet model\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Modify the first convolutional layer to accept 3 input channels\n",
    "        # self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.resnet.conv1 = nn.Conv2d(64, 64, kernel_size=(3, 3), padding=(1, 1), bias=False)\n",
    "\n",
    "        # Replace the final fully connected layer to match the number of classes\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Number of classes in your dataset\n",
    "num_classes = 10\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CustomResNet(num_classes=num_classes)\n",
    "\n",
    "# If you want to see the model summary for input size of 64x64 RGB images\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    torchsummary.summary(model, (64, 64, 3))\n",
    "else:\n",
    "    torchsummary.summary(model, (64, 64, 3))"
   ],
   "metadata": {
    "id": "1mCnw_IwN2p9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d9fcabf2-0fa4-450a-c6b4-0526a11e67e8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "training loop"
   ],
   "metadata": {
    "id": "8WGjtaUBRgRi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "# Init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# Set the model in training mode\n",
    "model.train()\n",
    "\n",
    "num_epochs = 1\n",
    "mini_batch_size = 2 # size of the mini-batches\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "from torch import optim\n",
    "learning_rate = 0.002\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train for n epochs\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Init collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "\n",
    "    train_loader_progress = tqdm.tqdm(train_dataloader)\n",
    "\n",
    "\n",
    "    # Update for each min-batch\n",
    "    for i, (x,y) in enumerate(train_loader_progress):\n",
    "\n",
    "        # TODO: Move to transforms\n",
    "        y = torch.tensor(y)\n",
    "        y = one_hot(y, 10).double()\n",
    "        x = torch.tensor(x)\n",
    "\n",
    "\n",
    "        # Transfer data to compute device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        #x = x.float()\n",
    "        pred = model(x)\n",
    "\n",
    "        # Reset model's gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = cross_entropy(pred, y)\n",
    "\n",
    "        # Run backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update network paramaters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store mini-batch losses\n",
    "        train_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "        train_loader_progress.set_description(f\"Loss: {loss.item():0.5f}\")\n",
    "\n",
    "        #np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    # Compute epoch loss\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch} - Loss: {train_epoch_loss:0.5f}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"checkpoint.pt\")"
   ],
   "metadata": {
    "id": "GQ-b3jIKRYjB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "976335a7-4c98-43e2-9f41-81e3db080711"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y.type()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Ff0pCK7aNZ7y",
    "outputId": "77f0cae1-9aef-4caa-d332-660a7280d6d1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "minibatch = next(enumerate(train_dataloader))"
   ],
   "metadata": {
    "id": "dzuMz3Z1lozP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "minibatch[1][0].shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJCOXbKHDtYe",
    "outputId": "1ef62364-74c3-4436-c9f2-8edcc70a9bfe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "evaluation loop"
   ],
   "metadata": {
    "id": "xl74eq_cRWOE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# set model in evaluation mode\n",
    "model.eval()\n",
    "\n"
   ],
   "metadata": {
    "id": "_j_RZDqEUqzb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "728823a6-c6c9-4508-b790-761dac70aedb",
    "ExecuteTime": {
     "start_time": "2024-03-26T13:48:46.075667Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "random_imgs = next(iter(test_dataloader))[0].to(\"cuda\")\n",
    "\n",
    "preds = model(random_imgs)\n",
    "\n",
    "def onehot2label(onehottensor):\n",
    "    num = torch.argmax(onehottensor).detach().item()\n",
    "    return train_dataset.ids2label[num]\n"
   ],
   "metadata": {
    "id": "VCWXM-EGasrm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "img_path = \"drive/MyDrive/AIML24/remote_sensing/otherDatasets/sentinel_2/testset/test_3310.npy\"\n",
    "\n",
    "def get_id(img_path):\n",
    "    return img_path.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "get_id(img_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tzjO6FnOd-z8",
    "outputId": "da79796c-c063-44fe-d2ac-a7743af0412b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "onehot2label(preds[2])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SYFT8aacb1Rj",
    "outputId": "c9c0de89-72d5-4b64-9070-7ecf3faa2cd9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classArray = []\n",
    "\n",
    "for i, (imgs, paths) in enumerate(test_dataloader):\n",
    "\n",
    "    img_ids = [get_id(path) for path in paths]\n",
    "    preds = model(imgs.to(\"cuda\"))\n",
    "    preds = list(preds.detach())\n",
    "    preds = [onehot2label(pred) for pred in preds]\n",
    "    classArray += list(zip(img_ids, preds))\n"
   ],
   "metadata": {
    "id": "7Q-IfruEb-f7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(classArray)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ct8HAdy8efab",
    "outputId": "9681bffa-d136-4087-925a-5f1a4ee1a4b8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Ziel: Array: classArray von (image index, predicted label)\n",
    "\n",
    "#loop durch test_dataloader (minibatch)\n",
    "    # pred labels (alle im minibatch auf einmal)\n",
    "    # find image ids for each pred ()\n",
    "    # Translate pred to text label\n",
    "    # append to array"
   ],
   "metadata": {
    "id": "3XxiAd7KYRnv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submission\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "7ObxBMEJM6gs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    ">We then create a csv with all our predictions to upload it to kaggle."
   ],
   "metadata": {
    "id": "9EwWNO6lMoPS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTl0h6xMMl4D"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('submission_v1.csv', 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['test_id', 'label'])\n",
    "\n",
    "    # Write each string to a row with its corresponding index as the test_id\n",
    "    for i, label in classArray:\n",
    "        writer.writerow([i, label])"
   ]
  }
 ]
}
