{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H24hLkOfNP8Z"
      },
      "source": [
        "# ML Coding Challenge\n",
        "## Random Forest Rangers\n",
        "## Urban, Atilla, Jano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2lLq2JG11yd"
      },
      "source": [
        "ssh -N -T -L 30001:localhost:30001 urban@dgx-2-02.ics.unisg.ch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZtLM40ANoYV"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-11T18:15:33.747304Z",
          "start_time": "2024-04-11T18:15:33.718280Z"
        },
        "id": "uFUdT5EvNjh5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PSMPxqggzmV"
      },
      "source": [
        "# Load Training & Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-11T18:19:32.046245Z",
          "start_time": "2024-04-11T18:19:32.025732Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE3L__QDgYvS",
        "outputId": "d8854479-444d-4538-cdc5-13945681707e"
      },
      "outputs": [],
      "source": [
        "# file locations\n",
        "DATA_PATH = \"drive/MyDrive/AIML24\"\n",
        "TRAINING_PATH = DATA_PATH + \"/remote_sensing/otherDatasets/sentinel_2/tif\"\n",
        "TEST_PATH = DATA_PATH + \"/remote_sensing/otherDatasets/sentinel_2/testset\"\n",
        "\n",
        "# mount g-drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # copy files into runtime for performance\n",
        "    !cp \"drive/MyDrive/AIML24\" \"AIML24\"\n",
        "    DATA_PATH = \"AIML24\"\n",
        "except:\n",
        "    pass # probably local environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-11T18:17:29.845750Z",
          "start_time": "2024-04-11T18:17:29.843397Z"
        },
        "id": "0SYpaK22lHsC"
      },
      "outputs": [],
      "source": [
        "# Download Training Data\n",
        "# !wget https://madm.dfki.de/files/sentinel/EuroSATallBands.zip --no-check-certificate\n",
        "# !unzip EuroSATallBands.zip\n",
        "# !mv ds/* \"drive/MyDrive/AIML24\"\n",
        "\n",
        "# ☝️Test data must be downloaded manually: put into AIML24/remote_sensing/otherDatasets/sentinel_2/testdata dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-11T18:15:44.541157Z",
          "start_time": "2024-04-11T18:15:44.526963Z"
        },
        "id": "EKZuqiBL7c3k"
      },
      "outputs": [],
      "source": [
        "# !unzip \"drive/MyDrive/AIML24/remote_sensing/otherDatasets/sentinel_2/testset.zip\"\n",
        "# !mv testset/testset/* \"drive/MyDrive/AIML24/remote_sensing/otherDatasets/sentinel_2/testset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq3pTziUr1MK"
      },
      "source": [
        "## Define PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-11T18:15:45.826100Z",
          "start_time": "2024-04-11T18:15:45.808195Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un57p0ZwhlaK",
        "outputId": "3f10f5e7-4eaa-46c5-a86d-260b619df994"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4232"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import glob\n",
        "len(glob.glob(os.path.join(TEST_PATH,  f\"*.npy\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-11T18:15:48.479745Z",
          "start_time": "2024-04-11T18:15:47.404369Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAZ7aZePhbJo",
        "outputId": "1f387d39-2b29-45fd-a067-b793ef6c8ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rasterio in /home/user/.venv/lib/python3.10/site-packages (1.3.10)\n",
            "Requirement already satisfied: attrs in /home/user/.venv/lib/python3.10/site-packages (from rasterio) (23.2.0)\n",
            "Requirement already satisfied: click-plugins in /home/user/.venv/lib/python3.10/site-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /home/user/.venv/lib/python3.10/site-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /home/user/.venv/lib/python3.10/site-packages (from rasterio) (1.4.7)\n",
            "Requirement already satisfied: click>=4.0 in /home/user/.venv/lib/python3.10/site-packages (from rasterio) (8.1.7)\n",
            "Requirement already satisfied: affine in /home/user/.venv/lib/python3.10/site-packages (from rasterio) (2.4.0)\n",
            "Requirement already satisfied: numpy in /home/user/.venv/lib/python3.10/site-packages (from rasterio) (1.26.4)\n",
            "Requirement already satisfied: certifi in /home/user/.venv/lib/python3.10/site-packages (from rasterio) (2024.2.2)\n",
            "Requirement already satisfied: setuptools in /home/user/.venv/lib/python3.10/site-packages (from rasterio) (65.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /home/user/.venv/lib/python3.10/site-packages (from snuggs>=1.4.1->rasterio) (3.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-11T18:18:37.269060Z",
          "start_time": "2024-04-11T18:18:37.237644Z"
        },
        "id": "U6j3IRM6gghb"
      },
      "outputs": [],
      "source": [
        "import rasterio as rio\n",
        "import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from rasterio.plot import reshape_as_image\n",
        "\n",
        "def load_img(img_path:str) -> np.ndarray:\n",
        "  if img_path.split('.')[-1] == \"tif\":\n",
        "    with rio.open(img_path, \"r\") as d:\n",
        "      img = d.read([1,2,3,4,5,6,7,8,9,10,11,12,13])\n",
        "      img = reshape_as_image(img)\n",
        "  else:\n",
        "    img = np.load(img_path)\n",
        "  return img\n",
        "\n",
        "\n",
        "ids2labels = {0: 'AnnualCrop',\n",
        " 1: 'Forest',\n",
        " 2: 'HerbaceousVegetation',\n",
        " 3: 'Highway',\n",
        " 4: 'Industrial',\n",
        " 5: 'Pasture',\n",
        " 6: 'PermanentCrop',\n",
        " 7: 'Residential',\n",
        " 8: 'River',\n",
        " 9: 'SeaLake'}\n",
        "\n",
        "labels2ids = {v: k for k, v in ids2labels.items()}\n",
        "\n",
        "def text2oh(label):\n",
        "    one_hot = np.zeros(10)\n",
        "    one_hot.put(labels2ids[label], 1)\n",
        "    return one_hot\n",
        "\n",
        "def oh2text(one_hot):\n",
        "    try:\n",
        "        one_hot = one_hot.numpy()\n",
        "    except:\n",
        "        pass\n",
        "    idx = np.argmax(one_hot)\n",
        "    return ids2labels[idx]\n",
        "\n",
        "class SentinelTrain(Dataset):\n",
        "    def __init__(self, transformations=None):\n",
        "        self.img_paths = [path.replace('\\\\', '/') for path in glob.glob(os.path.join(TRAINING_PATH, \"*\", f\"*.tif\"))]\n",
        "        self.transformations = transformations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = load_img(img_path).astype(np.float32)\n",
        "\n",
        "        if self.transformations:\n",
        "            image = self.transformations(image)\n",
        "\n",
        "        label = img_path.split(\"/\")[-1].split(\"_\")[0]\n",
        "        one_hot = text2oh(label)\n",
        "        # return image, one_hot\n",
        "        channels = torch.tensor([c for c in range(12)])\n",
        "        return image, {\"ID\": one_hot, \"channels\": channels}\n",
        "\n",
        "\n",
        "\n",
        "def get_id(img_path):\n",
        "    return img_path.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
        "\n",
        "class SentinelTest(Dataset):\n",
        "    def __init__(self, transformations=None):\n",
        "        self.img_paths = [path.replace(\"\\\\\",\"/\") for path in glob.glob(os.path.join(TEST_PATH,  f\"*.npy\"))]\n",
        "        self.transformations = transformations\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = load_img(img_path).astype(np.float32)\n",
        "        image_id = get_id(img_path)\n",
        "\n",
        "        if self.transformations:\n",
        "            image = self.transformations(image)\n",
        "        return image, image_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iO0y_kAgA4Vu"
      },
      "outputs": [],
      "source": [
        "# import math\n",
        "# data_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# # Compute the mean and std of the data\n",
        "# mean = 0.\n",
        "# std = 0.\n",
        "# nb_samples = 0.\n",
        "# for data in tqdm(data_loader):\n",
        "#     batch_samples = data[0].size(0)\n",
        "#     data = data[0].view(batch_samples, data[0].size(1), -1)\n",
        "#     mean += data.mean(2).sum(0)\n",
        "#     std += data.std(2).sum(0)\n",
        "#     nb_samples += batch_samples\n",
        "\n",
        "# mean /= nb_samples\n",
        "# std /= nb_samples\n",
        "\n",
        "# print(\"Mean Train:\", mean)\n",
        "# print(\"Std Train:\", std)\n",
        "\n",
        "# from tqdm import tqdm\n",
        "\n",
        "\n",
        "# print(\"Mean Test:\", mean)\n",
        "# print(\"Std Test:\", std)\n",
        "\n",
        "\n",
        "# def statistics(dataset):\n",
        "#     data_loader = DataLoader(dataset, batch_size=1)\n",
        "\n",
        "#     mean = 0.\n",
        "#     std = 0.\n",
        "#     min = 0.\n",
        "#     max = math.inf\n",
        "#     nb_samples = 0.\n",
        "\n",
        "#     for data in tqdm(data_loader):\n",
        "#         batch_samples = data[0].size(0)\n",
        "#         data = data[0].view(batch_samples, data[0].size(1), -1)\n",
        "#         mean += data.mean(2).sum(0)\n",
        "#         std += data.std(2).sum(0)\n",
        "#         min = \n",
        "#         nb_samples += batch_samples\n",
        "\n",
        "#     mean /= nb_samples\n",
        "#     std /= nb_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def statistics(dataset):\n",
        "    data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    mins = []\n",
        "    maxs = []\n",
        "    std = 0\n",
        "    mean = 0\n",
        "    n = 0\n",
        "\n",
        "    for data in tqdm(data_loader):\n",
        "        flat_bands = data[0][0].flatten(1)\n",
        "        mins.append(flat_bands.quantile(.1, 1))\n",
        "        maxs.append(flat_bands.quantile(.9, 1))\n",
        "        std += flat_bands.std(1)\n",
        "        mean += flat_bands.mean(1)\n",
        "        n += 1\n",
        "\n",
        "    min = torch.stack(mins).quantile(.1, 0)\n",
        "    max = torch.stack(maxs).quantile(.0, 0)\n",
        "    std = std / n\n",
        "    mean = mean / n\n",
        "\n",
        "    return min, max, std, mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m min_train, max_train, std_train, mean_train \u001b[38;5;241m=\u001b[39m statistics(\u001b[43mtrain_dataset\u001b[49m)\n\u001b[1;32m      2\u001b[0m min_test, max_test, std_test, mean_test \u001b[38;5;241m=\u001b[39m statistics(test_dataset)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "min_train, max_train, std_train, mean_train = statistics(train_dataset)\n",
        "min_test, max_test, std_test, mean_test = statistics(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-4.3008, -2.3040, -2.4507, -2.2064, -3.4897, -4.6369, -4.4517, -3.7969,\n",
              "        -4.6132, -6.4175, -4.5845, -3.5242])"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.stack(mins).quantile(.1, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MebCK7DA4Vu"
      },
      "source": [
        "### Tranforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-11T18:18:43.526453Z",
          "start_time": "2024-04-11T18:18:43.506219Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgWl6XyfnOrH",
        "outputId": "4e0d5e17-d646-44ca-e128-32be2f173510"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (2667543928.py, line 46)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[59], line 46\u001b[0;36m\u001b[0m\n\u001b[0;31m    Image.fromarray\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "# Create instances of the Dataset Class for both train & test\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "def bandselect(img):\n",
        "    return img[:, :, [3,2,1]]\n",
        "\n",
        "def convert_to_float(img):\n",
        "    return img / 10000.0\n",
        "\n",
        "def bands_to_front(img):\n",
        "    return img.permute(2, 0, 1)\n",
        "\n",
        "# TODO: improvement -> find global max / min\n",
        "def l2a_approx(img):\n",
        "    l2a_bands = img[:,:,[0,1,2,3,4,5,6,7,12,8,10,11]]\n",
        "    return l2a_bands\n",
        "    # band_min = np.min(l2a_bands, (0,1)) # minimal value per band\n",
        "    # return l2a_bands - band_min # dark object subtraction algo approximation\n",
        "\n",
        "train_mean = [1353.7283, 1117.2009, 1041.8888,  946.5547, 1199.1866, 2003.0106,\n",
        "        2374.0134, 2301.2244, 2599.7827,  732.1823, 1820.6930, 1118.2052]\n",
        "train_std = [ 65.2964, 153.7740, 187.6989, 278.1234, 227.9242, 355.9332, 455.1324,\n",
        "        530.7811, 502.1637,  98.9300, 378.1612, 303.1070]\n",
        "test_mean = [380.1732,  400.1498,  628.8646,  578.8707,  943.4276, 1826.2419,\n",
        "        2116.6646, 2205.9729, 2281.1836, 2266.9331, 1487.6902,  959.2352]\n",
        "test_std = [115.1743, 209.1482, 241.2069, 301.1053, 269.5137, 420.2494, 503.8187,\n",
        "        598.0409, 529.4133, 403.9382, 398.1438, 342.4408]\n",
        "\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    l2a_approx,\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(train_mean, train_std),\n",
        "    transforms.GaussianBlur(1),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "])\n",
        "\n",
        "train_dataset = SentinelTrain(train_transforms)\n",
        "\n",
        "\n",
        "test_transforms  = transforms.Compose([\n",
        "    Image.fromarray\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(test_mean, test_std),\n",
        "    transforms.GaussianBlur(1),\n",
        "])\n",
        "\n",
        "test_dataset = SentinelTest(test_transforms)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zy9guTzA4Vu"
      },
      "source": [
        "\n",
        "\n",
        "### Dataloaders (adjusted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-26T13:48:46.034706Z",
          "start_time": "2024-03-26T13:48:46.033705Z"
        },
        "id": "uJEWU_uJhf3E"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "# train_dataset_subset, val_dataset_subset = random_split(train_dataset, [.8, .2])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# val_dataloader = DataLoader(val_dataset_subset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "winfN1gXA4Vv"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-26T13:48:46.039237Z",
          "start_time": "2024-03-26T13:48:46.038036Z"
        },
        "id": "GJYIs5jInbbx"
      },
      "outputs": [],
      "source": [
        "# Utility function to display image from dataset\n",
        "from rasterio.plot import reshape_as_image\n",
        "\n",
        "def normalize_for_display(band_data):\n",
        "    \"\"\"Normalize multi-spectral imagery across bands.\n",
        "    The input is expected to be in HxWxC format, e.g. 64x64x13.\n",
        "    To account for outliers (e.g. extremly high values due to\n",
        "    reflective surfaces), we normalize with the 2- and 98-percentiles\n",
        "    instead of minimum and maximum of each band.\n",
        "    \"\"\"\n",
        "    band_data = np.array(band_data)\n",
        "    lower_perc = np.percentile(band_data, 2, axis=(0,1))\n",
        "    upper_perc = np.percentile(band_data, 98, axis=(0,1))\n",
        "\n",
        "    return (band_data - lower_perc) / (upper_perc - lower_perc)\n",
        "\n",
        "\n",
        "def print_image(img, label, rgb_bands=[3,2,1])-> None:\n",
        "  \"\"\"Displays an image. Indices of bands given by \"rgb_bands\" will be displayed as RGB in the print\n",
        "  \"\"\"\n",
        "  img = img.numpy()\n",
        "  img = img.transpose(1,2,0)\n",
        "  img = normalize_for_display(img)\n",
        "  img = img[:, :, rgb_bands]\n",
        "  fig, ax = plt.subplots(1, figsize=(5,5))\n",
        "  ax.imshow((img * 255).astype(np.uint8), vmin=0, vmax=255)\n",
        "\n",
        "  label = oh2text(label[\"ID\"])\n",
        "  ax.set_title(label)\n",
        "  ax.axis(False)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-03-26T13:48:46.048694Z",
          "start_time": "2024-03-26T13:48:46.047399Z"
        },
        "id": "rjQNNMvJrBsA"
      },
      "outputs": [],
      "source": [
        "# # example image from test\n",
        "# img, label = train_dataset.__getitem__(800)\n",
        "# print_image(img, label, [0,1,2])\n",
        "# # img.shape\n",
        "# #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDpb4f0SNs6x"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Ex4u0PA4Vv",
        "outputId": "ca2ad00d-0a66-4404-8c6c-d8593d300c6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "85487626"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import ViTForImageClassification, ViTConfig\n",
        "\n",
        "\n",
        "model = ViTForImageClassification(ViTConfig(\n",
        "    patch_size=3,\n",
        "    num_labels=10,\n",
        "    image_size=64,\n",
        "    num_channels=12\n",
        ")).to(\"cuda\")\n",
        "\n",
        "# Number of trainable parameters\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oqGuEnJNcJu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SPAIf2qA4Vw"
      },
      "source": [
        "Load previous Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVCiBPnzA4Vw"
      },
      "outputs": [],
      "source": [
        "# model = model.load_state_dict(torch.load(\"04162024-143828.pt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCNm48rYA4Vw"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "GQ-b3jIKRYjB",
        "outputId": "dd5e68c2-6c44-4e4a-bac7-b94544a4f8dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1/6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 2.544:   1%|▎                                                      | 2/352 [00:04<12:03,  2.07s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m train_mini_batch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     44\u001b[0m moving_avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y_dict \u001b[38;5;129;01min\u001b[39;00m (pbar \u001b[38;5;241m:=\u001b[39m tqdm(train_dataloader, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)):\n\u001b[1;32m     48\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmoving_avg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "Cell \u001b[0;32mIn[7], line 53\u001b[0m, in \u001b[0;36mSentinelTrain.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     52\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_paths[idx]\n\u001b[0;32m---> 53\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations:\n\u001b[1;32m     56\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations(image)\n",
            "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_img\u001b[39m(img_path:\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m img_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtif\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m d:\n\u001b[1;32m     10\u001b[0m       img \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mread([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m13\u001b[39m])\n\u001b[1;32m     11\u001b[0m       img \u001b[38;5;241m=\u001b[39m reshape_as_image(img)\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/rasterio/env.py:451\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    448\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/rasterio/__init__.py:304\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m path \u001b[38;5;241m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    306\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m get_writer_for_path(path, driver\u001b[38;5;241m=\u001b[39mdriver)(\n\u001b[1;32m    307\u001b[0m         path, mode, driver\u001b[38;5;241m=\u001b[39mdriver, sharing\u001b[38;5;241m=\u001b[39msharing, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    308\u001b[0m     )\n",
            "File \u001b[0;32mrasterio/_base.pyx:331\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mrasterio/_base.pyx:353\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase._set_attrs_from_dataset_handle\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mrasterio/_base.pyx:970\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.meta.__get__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mrasterio/_base.pyx:603\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.nodata.__get__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mrasterio/_base.pyx:584\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.nodatavals.__get__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mrasterio/_base.pyx:566\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.get_nodatavals\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/logging/__init__.py:1455\u001b[0m, in \u001b[0;36mLogger.debug\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m=\u001b[39m _checkLevel(level)\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanager\u001b[38;5;241m.\u001b[39m_clear_cache()\n\u001b[0;32m-> 1455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdebug\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m    Log 'msg % args' with severity 'DEBUG'.\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;124;03m    logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misEnabledFor(DEBUG):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "execution_start = datetime.now().strftime(\"%m%d%Y-%H%M%S\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.005\n",
        "num_epochs = 1\n",
        "\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "model.train()\n",
        "\n",
        "\n",
        "\n",
        "k_folds = 6\n",
        "\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "val_cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "    print(f\"Fold {fold+1}/{k_folds}\")\n",
        "\n",
        "    # Create data loaders for the current fold\n",
        "    train_subsampler = Subset(train_dataset, train_ids)\n",
        "    val_subsampler = Subset(train_dataset, test_ids)\n",
        "\n",
        "    train_dataloader = DataLoader(train_subsampler, batch_size=64, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_subsampler, batch_size=64, shuffle=True)\n",
        "\n",
        "    train_losses = []\n",
        "    train_epoch_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_mini_batch_losses = []\n",
        "        moving_avg_loss = 0.0\n",
        "\n",
        "        for x, y_dict in (pbar := tqdm(train_dataloader, ncols=100)):\n",
        "\n",
        "            pbar.set_description(f\"{moving_avg_loss: .3f}\")\n",
        "            x = x.float().to(device)\n",
        "            y = y_dict['ID'].to(device)\n",
        "\n",
        "            outputs = model(x)\n",
        "            loss = cross_entropy(outputs.logits, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Store and log mini-batch losses\n",
        "            train_mini_batch_losses.append(loss.item())\n",
        "            train_losses.append(loss.item())\n",
        "            moving_avg_loss = np.mean(train_losses[-5:])\n",
        "\n",
        "        else:\n",
        "            epoch_loss = np.mean(train_mini_batch_losses)\n",
        "            pbar.set_description(f\"{epoch_loss: .3f}\")\n",
        "            train_epoch_losses.append(epoch_loss)\n",
        "\n",
        "        if epoch_loss < 0.75:\n",
        "            break\n",
        "\n",
        "    # model.eval()\n",
        "    validation_losses = []\n",
        "    with torch.no_grad():\n",
        "        for x, y_dict in val_dataloader:\n",
        "            x = x.float().to(device)\n",
        "            y = y_dict['ID'].to(device)\n",
        "            outputs = model(x)\n",
        "            loss = val_cross_entropy(outputs.logits, y)\n",
        "            # optimizer.zero_grad()\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            validation_losses.append(loss.item())\n",
        "\n",
        "    avg_val_loss = np.mean(validation_losses)\n",
        "    print(f\"Validation Loss for Fold {fold+1}: {avg_val_loss:.3f}\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), f\"{execution_start}.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "\n",
        "# model = best_model\n",
        "# torch.save(model.state_dict(), f\"{execution_start}.pt\")\n",
        "\n",
        "# print(best_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQKFot0J11y8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "\n",
        "p = sns.lineplot(train_epoch_losses)\n",
        "p.set(yscale='log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQP8Ikcp11y-"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-03-26T13:48:46.075667Z"
        },
        "id": "_j_RZDqEUqzb"
      },
      "outputs": [],
      "source": [
        "# # set model in evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "\n",
        "# validate = []\n",
        "# for i, (imgs, labels) in enumerate(val_dataloader):\n",
        "#     preds = model(imgs.to(\"cuda\")).logits\n",
        "#     pred_labels = [oh2text(oh.detach().cpu()) for oh in preds]\n",
        "#     validate += list(zip(labels, preds))\n",
        "\n",
        "\n",
        "# # TODO: Test Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtxx5AGS11zA"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl74eq_cRWOE"
      },
      "source": [
        "evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q-IfruEb-f7"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "classArray = []\n",
        "\n",
        "for i, (imgs, paths) in enumerate(test_dataloader):\n",
        "\n",
        "    img_ids = [get_id(path) for path in paths]\n",
        "    preds = model(imgs.to(\"cuda\"))\n",
        "    preds = list(preds.logits.detach())\n",
        "    preds = [oh2text(pred.detach().cpu()) for pred in preds]\n",
        "    classArray += list(zip(img_ids, preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ObxBMEJM6gs"
      },
      "source": [
        "## Submission\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTl0h6xMMl4D"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open(f'{execution_start}.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['test_id', 'label'])\n",
        "\n",
        "    # Write each string to a row with its corresponding index as the test_id\n",
        "    for i, label in classArray:\n",
        "        writer.writerow([i, label])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
