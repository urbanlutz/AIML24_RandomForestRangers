{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H24hLkOfNP8Z"
   },
   "source": [
    "# ML Coding Challenge\n",
    "## Random Forest Rangers\n",
    "## Urban, Atilla, Jano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05IlVmr7sA5J"
   },
   "source": [
    "ssh -N -T -L 30001:localhost:30001 urban@dgx-2-02.ics.unisg.ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZtLM40ANoYV"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:47:07.487443Z",
     "start_time": "2024-03-26T13:47:04.313237Z"
    },
    "id": "uFUdT5EvNjh5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PSMPxqggzmV"
   },
   "source": [
    "# Load Training & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:47:07.505475Z",
     "start_time": "2024-03-26T13:47:07.491462Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kE3L__QDgYvS",
    "outputId": "ee25fc6b-ab9a-4fc3-832f-38e6eb694436"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "cp: -r not specified; omitting directory 'drive/MyDrive/AIML24'\n"
     ]
    }
   ],
   "source": [
    "# file locations\n",
    "DATA_PATH = \"drive/MyDrive/AIML24\"\n",
    "TRAINING_PATH = DATA_PATH + \"/remote_sensing/otherDatasets/sentinel_2/tif\"\n",
    "TEST_PATH = DATA_PATH + \"/remote_sensing/otherDatasets/sentinel_2/testset\"\n",
    "\n",
    "# mount g-drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # copy files into runtime for performance\n",
    "    !cp \"drive/MyDrive/AIML24\" \"AIML24\"\n",
    "    DATA_PATH = \"AIML24\"\n",
    "except:\n",
    "    pass # probably local environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:47:07.519630Z",
     "start_time": "2024-03-26T13:47:07.509853Z"
    },
    "id": "0SYpaK22lHsC"
   },
   "outputs": [],
   "source": [
    "# Download Training Data\n",
    "# !wget https://madm.dfki.de/files/sentinel/EuroSATallBands.zip --no-check-certificate\n",
    "# !unzip EuroSATallBands.zip\n",
    "# !mv ds/* \"drive/MyDrive/AIML24\"\n",
    "\n",
    "# ☝️Test data must be downloaded manually: put into AIML24/remote_sensing/otherDatasets/sentinel_2/testdata dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:47:07.546862Z",
     "start_time": "2024-03-26T13:47:07.538399Z"
    },
    "id": "EKZuqiBL7c3k"
   },
   "outputs": [],
   "source": [
    "# !unzip \"drive/MyDrive/AIML24/remote_sensing/otherDatasets/sentinel_2/testset.zip\"\n",
    "# !mv testset/testset/* \"drive/MyDrive/AIML24/remote_sensing/otherDatasets/sentinel_2/testset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq3pTziUr1MK"
   },
   "source": [
    "## Define PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:13.256735Z",
     "start_time": "2024-03-26T13:48:13.247073Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "un57p0ZwhlaK",
    "outputId": "a09ed06d-3cbe-4ba5-dce0-db82606376f3"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4232"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import glob\n",
    "len(glob.glob(os.path.join(TEST_PATH,  f\"*.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:32.891115Z",
     "start_time": "2024-03-26T13:48:13.419490Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAZ7aZePhbJo",
    "outputId": "4e2dab85-d4e8-4a13-a087-940993faf699"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting rasterio\n",
      "  Downloading rasterio-1.3.10-cp310-cp310-manylinux2014_x86_64.whl (21.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.5/21.5 MB\u001B[0m \u001B[31m53.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting affine (from rasterio)\n",
      "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2024.2.2)\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.25.2)\n",
      "Collecting snuggs>=1.4.1 (from rasterio)\n",
      "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.2)\n",
      "Installing collected packages: snuggs, affine, rasterio\n",
      "Successfully installed affine-2.4.0 rasterio-1.3.10 snuggs-1.4.7\n"
     ]
    }
   ],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T15:57:18.582403Z",
     "start_time": "2024-03-26T15:57:18.566479Z"
    },
    "id": "U6j3IRM6gghb"
   },
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "from collections.abc import Callable\n",
    "\n",
    "\n",
    "def load_img(img_path:str) -> np.ndarray:\n",
    "  if img_path.split('.')[-1] == \"tif\":\n",
    "    with rio.open(img_path, \"r\") as d:\n",
    "      img = d.read([1,2,3,4,5,6,7,8,9,10,11,12,13])\n",
    "      # Assuming bands 2, 3, 4 are the RGB channels (1-based indexing in rasterio)\n",
    "      # Adjust the indices as necessary based on your data\n",
    "      # img = d.read([2, 3, 4])\n",
    "      img = reshape_as_image(img)\n",
    "  else:\n",
    "    img = np.load(img_path)\n",
    "  return img\n",
    "\n",
    "\n",
    "ids2labels = {0: 'AnnualCrop',\n",
    " 1: 'Forest',\n",
    " 2: 'HerbaceousVegetation',\n",
    " 3: 'Highway',\n",
    " 4: 'Industrial',\n",
    " 5: 'Pasture',\n",
    " 6: 'PermanentCrop',\n",
    " 7: 'Residential',\n",
    " 8: 'River',\n",
    " 9: 'SeaLake'}\n",
    "\n",
    "labels2ids = {v: k for k, v in ids2labels.items()}\n",
    "\n",
    "def text2oh(label):\n",
    "    one_hot = np.zeros(10)\n",
    "    one_hot.put(labels2ids[label], 1)\n",
    "    return one_hot\n",
    "\n",
    "def oh2text(one_hot):\n",
    "    try:\n",
    "        one_hot = one_hot.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    idx = np.argmax(one_hot)\n",
    "    return ids2labels[idx]\n",
    "\n",
    "class SentinelTrain(Dataset):\n",
    "    def __init__(self, transformations=None):\n",
    "        self.img_paths = [path.replace('\\\\', '/') for path in glob.glob(os.path.join(TRAINING_PATH, \"*\", f\"*.tif\"))]\n",
    "\n",
    "        # labels = set([path.split('/')[-1].split('_')[0] for path in self.img_paths]) # get unique labels\n",
    "        # self.label2ids = {name: id for (id, name) in enumerate(sorted(labels))} # map an id to each label\n",
    "        # self.ids2label = {v:k for k, v in self.label2ids.items()} # reverse lookup dict\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = load_img(img_path)\n",
    "\n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "\n",
    "        label = img_path.split(\"/\")[-1].split(\"_\")[0]\n",
    "        one_hot = text2oh(label)\n",
    "        return image, one_hot\n",
    "\n",
    "\n",
    "\n",
    "def get_id(img_path):\n",
    "    return img_path.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "class SentinelTest(Dataset):\n",
    "    def __init__(self, transformations=None):\n",
    "        self.img_paths = [path.replace(\"\\\\\",\"/\") for path in glob.glob(os.path.join(TEST_PATH,  f\"*.npy\"))]\n",
    "        self.transformations = transformations\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = load_img(img_path)\n",
    "        image_id = get_id(img_path)\n",
    "\n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "        return image, image_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybtNb-DbsA5N",
    "outputId": "8545b94b-f6d7-4d89-e8d0-5882429d8363",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.2.1+cu121)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->torchvision)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T15:57:21.365738Z",
     "start_time": "2024-03-26T15:57:21.184984Z"
    },
    "id": "rgWl6XyfnOrH"
   },
   "outputs": [],
   "source": [
    "# Create instances of the Dataset Class for both train & test\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def bandselect(img):\n",
    "    return img[:, :, [3,2,1]]\n",
    "\n",
    "def convert_to_float(img):\n",
    "    return img.astype(np.float32) / 10000.0\n",
    "\n",
    "def permute(img):\n",
    "    return img.transpose(2, 0, 1)\n",
    "\n",
    "# TODO: improvement -> find global max / min\n",
    "def l2a_approx(img):\n",
    "    l2a_bands = img[:,:,[0,1,2,3,4,5,6,7,8,9,11,12]]\n",
    "    band_min = np.min(l2a_bands, (0,1)) # minimal value per band\n",
    "    return l2a_bands - band_min # dark object subtraction algo approximation\n",
    "\n",
    "# Define the transformation functions as lambdas or wrap in Lambda transform\n",
    "train_transforms = transforms.Compose([\n",
    "   l2a_approx,\n",
    "    # bandselect,\n",
    "    convert_to_float,\n",
    "    permute\n",
    "])\n",
    "train_dataset = SentinelTrain(train_transforms)\n",
    "\n",
    "\n",
    "test_transforms  = transforms.Compose([\n",
    "    # bandselect,\n",
    "    convert_to_float,\n",
    "    permute\n",
    "])\n",
    "test_dataset = SentinelTest(test_transforms)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:46.034706Z",
     "start_time": "2024-03-26T13:48:46.033705Z"
    },
    "id": "uJEWU_uJhf3E"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "train_dataset_subset, val_dataset_subset = random_split(train_dataset, [.8, .2])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset_subset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset_subset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeojguXWsA5O"
   },
   "outputs": [],
   "source": [
    "ids2labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:46.039237Z",
     "start_time": "2024-03-26T13:48:46.038036Z"
    },
    "id": "GJYIs5jInbbx"
   },
   "outputs": [],
   "source": [
    "# Utility function to display image from dataset\n",
    "from rasterio.plot import reshape_as_image\n",
    "\n",
    "def normalize_for_display(band_data):\n",
    "    \"\"\"Normalize multi-spectral imagery across bands.\n",
    "    The input is expected to be in HxWxC format, e.g. 64x64x13.\n",
    "    To account for outliers (e.g. extremly high values due to\n",
    "    reflective surfaces), we normalize with the 2- and 98-percentiles\n",
    "    instead of minimum and maximum of each band.\n",
    "    \"\"\"\n",
    "    band_data = np.array(band_data)\n",
    "    lower_perc = np.percentile(band_data, 2, axis=(0,1))\n",
    "    upper_perc = np.percentile(band_data, 98, axis=(0,1))\n",
    "    print(lower_perc)\n",
    "    print(upper_perc)\n",
    "    return (band_data - lower_perc) / (upper_perc - lower_perc)\n",
    "\n",
    "\n",
    "def print_image(img, label, rgb_bands=[3,2,1])-> None:\n",
    "  \"\"\"Displays an image. Indices of bands given by \"rgb_bands\" will be displayed as RGB in the print\n",
    "  \"\"\"\n",
    "  img = img.transpose(1,2,0)\n",
    "  img = normalize_for_display(img)\n",
    "  img = img[:, :, rgb_bands]\n",
    "  fig, ax = plt.subplots(1, figsize=(5,5))\n",
    "  ax.imshow((img * 255).astype(np.uint8), vmin=0, vmax=255)\n",
    "  # ax.imshow(img)\n",
    "  ax.set_title(label)\n",
    "  ax.axis(False)\n",
    "  plt.tight_layout()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:46.043862Z",
     "start_time": "2024-03-26T13:48:46.042405Z"
    },
    "id": "zmFUg37hoQdf"
   },
   "outputs": [],
   "source": [
    "# Example image from train\n",
    "img, label = train_dataset.__getitem__(120)\n",
    "print_image(img, oh2text(label), [0,1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkqQd0vvsA5P"
   },
   "outputs": [],
   "source": [
    "oh2text(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mXyGWb4sA5P"
   },
   "outputs": [],
   "source": [
    "labels2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kyi3sYhssA5P"
   },
   "outputs": [],
   "source": [
    "train_dataset.img_paths[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:48:46.048694Z",
     "start_time": "2024-03-26T13:48:46.047399Z"
    },
    "id": "rjQNNMvJrBsA"
   },
   "outputs": [],
   "source": [
    "# example image from test\n",
    "img, label = test_dataset.__getitem__(800)\n",
    "print_image(img, label, [0,1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDpb4f0SNs6x"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ooY1wBiCWBN"
   },
   "outputs": [],
   "source": [
    "img.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FYoXQYMsA5Q"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "torchvision.models.vit_b_16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFnJNk9nTCnk"
   },
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mCnw_IwN2p9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchsummary import torchsummary\n",
    "\n",
    "def get_num_channels():\n",
    "    img, _ = test_dataset.__getitem__(0)\n",
    "    return img.shape[0]\n",
    "\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        # Load a pre-trained ResNet model\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Modify the first convolutional layer to accept 3 input channels\n",
    "        self.resnet.conv1 = nn.Conv2d(get_num_channels(), 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        # self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), padding=(1, 1), bias=False)\n",
    "\n",
    "        # Replace the final fully connected layer to match the number of classes\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Number of classes in your dataset\n",
    "num_classes = 10\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CustomResNet(num_classes=num_classes)\n",
    "\n",
    "# If you want to see the model summary for input size of 64x64 RGB images\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    torchsummary.summary(model, (get_num_channels(), 64, 64))\n",
    "else:\n",
    "    torchsummary.summary(model, (get_num_channels(), 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JY_3g5UhsA5R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WGjtaUBRgRi"
   },
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "setup training loop"
   ],
   "metadata": {
    "id": "xtc0BvkOWVd0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Init collection of training epoch losses\n",
    "train_losses = []\n",
    "train_epoch_losses = []\n",
    "\n",
    "# Set the model in training mode\n",
    "model.train()\n",
    "\n",
    "num_epochs = 12\n",
    "mini_batch_size = 64 # size of the mini-batches\n",
    "\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "from torch import optim\n",
    "learning_rate = 0.002\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "id": "Lo9AXnc9WYIH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "if model should start after a checkpoint, checkpoint always after an epoch."
   ],
   "metadata": {
    "id": "W789KLhYWb48"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load checkpoint after full epoch, when training was interrupted\n",
    "\n",
    "# checkpoint = torch.load('checkpoint_epoch_X.pth')  # Replace X with the desired epoch\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# start_epoch = checkpoint['epoch']  # Start from the next epoch\n",
    "# last_loss = checkpoint['loss']\n",
    "# model.train()"
   ],
   "metadata": {
    "id": "g08T3ePDWjQL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQ-b3jIKRYjB"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train for the next epoch from checkpoint\n",
    "# for epoch in range(start_epoch, num_epochs):\n",
    "# Train for n epochs\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Init collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "\n",
    "    train_loader_progress = tqdm.tqdm(train_dataloader)\n",
    "\n",
    "\n",
    "    # Update for each min-batch\n",
    "    for i, (x,y) in enumerate(train_loader_progress):\n",
    "        # Transfer data to compute device\n",
    "        x, y = torch.tensor(x).to(device), torch.tensor(y).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        #x = x.float()\n",
    "        pred = model(x)\n",
    "\n",
    "        # Reset model's gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = cross_entropy(pred, y)\n",
    "\n",
    "        # Run backward pass\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Update network paramaters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store mini-batch losses\n",
    "        train_mini_batch_losses.append(loss.data.item())\n",
    "        train_losses.append(loss.data.item())\n",
    "\n",
    "        train_loader_progress.set_description(f\"Loss: {loss.item():0.5f}\")\n",
    "\n",
    "        #np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    # Compute epoch loss\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch} - Loss: {train_epoch_loss:0.5f}\")\n",
    "\n",
    "    # Save checkpoint at each epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': train_epoch_loss,\n",
    "    }, f\"checkpoint_epoch_{epoch}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x95mjTAzsA5R"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "p = sns.lineplot(train_losses)\n",
    "p.set(yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgu3mlW7sA5S"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load checkpoint, when using trained model as start for eval\n",
    "\n",
    "# checkpoint = torch.load('checkpoint_epoch_X.pth')  # Replace X with the desired epoch\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# start_epoch = checkpoint['epoch']  # Start from the next epoch\n",
    "# last_loss = checkpoint['loss']"
   ],
   "metadata": {
    "id": "RFpOLYEDW-Cn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-26T13:48:46.075667Z"
    },
    "id": "_j_RZDqEUqzb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "validate = []\n",
    "\n",
    "for i, (imgs, labels) in enumerate(val_dataloader):\n",
    "    preds = model(imgs.to(\"cuda\"))\n",
    "    pred_labels = [oh2text(oh.detach().cpu()) for oh in preds]\n",
    "    validate += list(zip(labels, preds))\n",
    "\n",
    "\n",
    "# TODO: Test Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxWNLKXcsA5S"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xl74eq_cRWOE"
   },
   "source": [
    "evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzjO6FnOd-z8"
   },
   "outputs": [],
   "source": [
    "img_path = \"drive/MyDrive/AIML24/remote_sensing/otherDatasets/sentinel_2/testset/test_3310.npy\"\n",
    "\n",
    "def get_id(img_path):\n",
    "    return img_path.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "\n",
    "get_id(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Q-IfruEb-f7"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "classArray = []\n",
    "\n",
    "for i, (imgs, paths) in enumerate(test_dataloader):\n",
    "\n",
    "    img_ids = [get_id(path) for path in paths]\n",
    "    preds = model(imgs.to(\"cuda\"))\n",
    "    preds = list(preds.detach())\n",
    "    preds = [oh2text(pred.detach().cpu()) for pred in preds]\n",
    "    classArray += list(zip(img_ids, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ct8HAdy8efab"
   },
   "outputs": [],
   "source": [
    "len(classArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XxiAd7KYRnv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ObxBMEJM6gs"
   },
   "source": [
    "## Submission\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EwWNO6lMoPS"
   },
   "source": [
    ">We then create a csv with all our predictions to upload it to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTl0h6xMMl4D"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('submission_v1.csv', 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['test_id', 'label'])\n",
    "\n",
    "    # Write each string to a row with its corresponding index as the test_id\n",
    "    for i, label in classArray:\n",
    "        writer.writerow([i, label])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
